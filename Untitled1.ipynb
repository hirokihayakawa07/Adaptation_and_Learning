{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graham Traines\n",
    "# CSCI 7090 Machine Learning\n",
    "# Logistic Regression with Gradient Descent\n",
    "# Demonstration\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as mpl\n",
    "\n",
    "\"\"\"\n",
    "GO TO THE BOTTOM OF THE SCRIPT TO FIND THE START\n",
    "\"\"\"\n",
    "\n",
    "# method:\n",
    "# ---Load data\n",
    "def load_data_set(file_location, delimiter, column_y, column_x1):\n",
    "    \"\"\"\n",
    "    :param file_location: string for data file location\n",
    "    :param delimiter: ',' for CSV, etc.\n",
    "    :param column_y: the column containing the target values\n",
    "    :param column_x1: input data column -- right now, I only take 1\n",
    "    TODO: add a parameter that is a set so I can take multiple input columns\n",
    "    :return: Numpy Array of target values, input values, and bias term (bias term always = 1.0)\n",
    "    \"\"\"\n",
    "    data = sp.genfromtxt(file_location, delimiter=delimiter, dtype=None)\n",
    "\n",
    "    # Need to get everything after the headers\n",
    "\n",
    "    X = data[1:, column_x1]\n",
    "    Y = data[1:, column_y]\n",
    "\n",
    "    # we make the cases 1 and -1 to fit with the Likelihood Formula\n",
    "    # P(y | x) -> h(x) for y = +1\n",
    "    # P(y | x) -> 1 - h(x) for y = -1\n",
    "    y_numeric = [1.0 if entry == 'Yes' else -1.0 for entry in Y]\n",
    "\n",
    "    # Will use this for x0\n",
    "    ones = [1.0 for x in X]\n",
    "\n",
    "    return np.array(zip(y_numeric, X, ones), dtype='float_')\n",
    "\n",
    "def sigmoid(weights, x):\n",
    "    \"\"\"\n",
    "    Sigmoid function as presented in the CalTech lectures\n",
    "    I make it overly verbose so I can see what's going on inside\n",
    "    :param weights: vector of value weights, including bias weight\n",
    "    :param x: vector of input values, including bias term\n",
    "    :return: a number between 0 and 1\n",
    "    \"\"\"\n",
    "    sigmoid_func = lambda s: np.exp(s) / (1.0 + np.exp(s))\n",
    "    T = weights.transpose().dot(x)\n",
    "    theta_s = sigmoid_func(T)\n",
    "    return theta_s\n",
    "\n",
    "\n",
    "def print_sigmoids(weights, x_values):\n",
    "    \"\"\"\n",
    "    For debugging/testing, look directly at the values instead of plotting\n",
    "    :param weights: vector of value weights, including bias weight\n",
    "    :param x: vector of input values, including bias term\n",
    "    :return: void\n",
    "    \"\"\"\n",
    "\n",
    "    for x in x_values:\n",
    "        print( x, str(sigmoid(weights, x)))\n",
    "\n",
    "    return\n",
    "\n",
    "def plot_sigmoid(weights, x_values, x_label, iterations):\n",
    "    \"\"\"\n",
    "    Plots probability vs. our random variable\n",
    "    :param weights: Vector of weights including bias weight\n",
    "    :param x_values: Array of X values, including bias term\n",
    "    :param x_label: Name of the data column we are learning\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_label = 'Probability of True'\n",
    "\n",
    "    y_values = []\n",
    "    for x in x_values:\n",
    "        y_values.append(sigmoid(weights, x))\n",
    "\n",
    "    x_space = x_values[:, 0]\n",
    "    y_space = np.array(y_values)\n",
    "\n",
    "    mpl.plot(x_space, y_space, 'ro')\n",
    "    mpl.title(\"Sigmoid Function for Probability of Default Given \" + x_label + \" After \" + str(iterations) + \" Iterations\")\n",
    "    mpl.xlabel(x_label)\n",
    "    mpl.ylabel(y_label)\n",
    "    mpl.autoscale(tight=True)\n",
    "    mpl.grid()\n",
    "    mpl.show()\n",
    "    return\n",
    "\n",
    "def plot_data_set(x_values, y_values, x_label):\n",
    "    \"\"\"\n",
    "    :param x_values: array of x values, including bias term (for simplicity, I remove it in here\n",
    "    :param y_values: array of target values\n",
    "    :param x_label: name of the data we are considering\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    y_label = 'Defaulted (yes/no)'\n",
    "\n",
    "    x_space = x_values[:, 0]\n",
    "    y_space = np.array(y_values)\n",
    "\n",
    "    mpl.plot(x_space, y_space, 'go')\n",
    "    mpl.title(\"Data for \" + x_label + \" Compared to Defaults\")\n",
    "    mpl.xlabel(x_label)\n",
    "    mpl.ylabel(y_label)\n",
    "    mpl.autoscale(tight=True)\n",
    "    mpl.grid()\n",
    "    mpl.show()\n",
    "    return\n",
    "\n",
    "\n",
    "#method:\n",
    "#---Gradient descent\n",
    "def gradient_descent(little_epsilon, eta, y_set, x_set, iterations):\n",
    "    \"\"\"\n",
    "    :param little_epsilon: acceptable error rate\n",
    "    :param eta: learning step size\n",
    "    :param y_set: set of target values\n",
    "    :param x_set: set of input values, including bias term (1.0)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert type(y_set) is np.ndarray, \"y_set is not numpy array\"\n",
    "    assert type(x_set) is np.ndarray, \"x_set is not numpy array\"\n",
    "\n",
    "    # t = number of iterations\n",
    "    t = 0\n",
    "    # initialize w(0) to 0\n",
    "    weights_t = np.array([0.0, 0.0])\n",
    "\n",
    "    # depending on the data set, we could continue training until\n",
    "    # the gradient is smaller than our acceptable epsilon\n",
    "    # otherwise, not using the little_epsilon parameter at the moment\n",
    "    #while abs(grEin.any()) >= little_epsilon:\n",
    "\n",
    "    # iterate to the next step until it is time to stop\n",
    "    for i in range(0, iterations):\n",
    "        # grEin = gradient of in-sample Error\n",
    "        grEin = get_gradient(y_set, weights_t, x_set)\n",
    "\n",
    "        # update the weights\n",
    "        # delta w = eta * grEin\n",
    "        delta = (1.0 * eta) * grEin\n",
    "\n",
    "        # w(t + 1) = w(t) - delta(w(t))\n",
    "        weights_t = weights_t - delta\n",
    "\n",
    "        # The following block is for debugging/testing purposes\n",
    "        # print delta\n",
    "        #print abs(grEin)\n",
    "        #print grEin\n",
    "        #print eta * grEin\n",
    "        #print weights_t\n",
    "\n",
    "        # update iteration count\n",
    "        t += 1\n",
    "\n",
    "    # return the final weights vector W (weights_t)\n",
    "    return weights_t\n",
    "\n",
    "\n",
    "#method:\n",
    "#---Get gradient at this t with w(t)\n",
    "def get_gradient(y_set, weights_t, data_set):\n",
    "    \"\"\"\n",
    "    :param y_set: the array of target values\n",
    "    :param weights_t: array of weights with bias term\n",
    "    :param data_set: array of training values with bias term\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(y_set) is np.ndarray, \"y_set is not a numpy array\"\n",
    "    assert type(weights_t) is np.ndarray, \"weights_t is not a numpy array\"\n",
    "    assert type(data_set) is np.ndarray, \"dataset is not a numpy matrix\"\n",
    "\n",
    "    # number of training examples\n",
    "    N = len(data_set)\n",
    "    # get the sum of error in the whole set\n",
    "    summation_error = 0.0\n",
    "    for n in range(0, N):\n",
    "        Xn = np.array(data_set[n, :])\n",
    "        summation_error += get_partial_gradient_sum(y_set[n], weights_t, Xn)\n",
    "\n",
    "    # average out the error\n",
    "    # gradient of in-sample error = -1/N * (summation of n in N -> (yn*Xn) / (1 + euler's constant ^ (yn * W(t).T * Xn))\n",
    "    gradient = (-1.0/N) * summation_error\n",
    "    return gradient\n",
    "\n",
    "\n",
    "# method:\n",
    "# returns the value for n : yn * Xn / (1 + e^(yn * W(t).T * Xn)\n",
    "def get_partial_gradient_sum(yn, Wt, Xn):\n",
    "    '''\n",
    "    This is the inner term of the summation from the CalTech lectures/text\n",
    "    I separate it out for readability/inspection/debugging\n",
    "    It should really be in a matrix operation, this is really slow\n",
    "    :param yn: target value for data row in question\n",
    "    :param Wt: vector of Weights for this iteration\n",
    "    :param Xn: vector of inputs for data row\n",
    "    :return: Computed value for the summation term  yn * Xn / (1 + e^(yn * W(t).T * Xn)\n",
    "    '''\n",
    "    #assert yn is np.ndarray, \"y is not a numpy array\"\n",
    "    #assert Xn is np.ndarray, \"Xn is not a numpy array\"\n",
    "    #assert Wt is np.ndarray, \"Wt is not a numpy array\"\n",
    "    #print Wt\n",
    "    #print yn\n",
    "\n",
    "    # y[n]X[n]\n",
    "    top_term = yn * Xn\n",
    "    # y[n] * W[t]T * X[n]\n",
    "    exponent_term = yn * (Wt.transpose().dot(Xn))\n",
    "    # 1 + e^exponent\n",
    "    divisor = 1.0 + np.exp(exponent_term)\n",
    "    partial = top_term / divisor\n",
    "\n",
    "    return partial\n",
    "\n",
    "\n",
    "#method:\n",
    "#---Execute the learning and show results\n",
    "def learn_class(data_set_file_location, data_column, column_name, target_column, little_epsilon, eta, iterations):\n",
    "    \"\"\"\n",
    "    :param data_set_file_location: Location of data file to load\n",
    "    :param data_column: Integer for input data column in the data file\n",
    "    :param column_name: Name of the data we are considering (just a string we make up)\n",
    "    :param target_column: Integer for target data column in the data file\n",
    "    :param little_epsilon: Tolerable in-sample error (not currently being used)\n",
    "    :param eta: Learning step size\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    global dataset, classes\n",
    "    # Load rows from credit_data.csv\n",
    "    # Row 1 = Target, Row 3 = Balance, Row 4 = Income\n",
    "    dataset = load_data_set(data_set_file_location, \",\", target_column, data_column)\n",
    "\n",
    "    # get possible classes (should be -1 and 1)\n",
    "    # in case we want to check the data\n",
    "    # classes = np.unique(dataset[:, 0])\n",
    "    # for d in dataset:\n",
    "    #     if d[0] == 1:\n",
    "    #         print d\n",
    "\n",
    "    X = np.array(dataset[:, 1:])\n",
    "    if data_column == 4:\n",
    "        X = np.array([x/1000.0 for x in X])\n",
    "    Y_set = np.array(dataset[:, 0])\n",
    "\n",
    "    plot_data_set(X, Y_set, column_name)\n",
    "\n",
    "    #learn the functions\n",
    "    weights = gradient_descent(little_epsilon, eta, Y_set, X, iterations)\n",
    "\n",
    "    #return weights, intercept\n",
    "    print( \"Coefficient: \", weights[0])\n",
    "    print( \"Intercept: \", weights[1])\n",
    "\n",
    "    #print_sigmoids(weights, sgd_classifier.intercept_, XT)\n",
    "    plot_sigmoid(weights, X, column_name, iterations)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/linux_home/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a81349ed6384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# input data column: 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#learn: P(Default|Balance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mlearn_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Credit_Data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Balance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlittle_epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# target data column: 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-483e661bf39d>\u001b[0m in \u001b[0;36mlearn_class\u001b[0;34m(data_set_file_location, data_column, column_name, target_column, little_epsilon, eta, iterations)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;31m# Load rows from credit_data.csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;31m# Row 1 = Target, Row 3 = Balance, Row 4 = Income\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_file_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# get possible classes (should be -1 and 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-483e661bf39d>\u001b[0m in \u001b[0;36mload_data_set\u001b[0;34m(file_location, delimiter, column_y, column_x1)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'zip'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "!!!!!!!!!!!!!!!!!!\n",
    "!!! START HERE !!!\n",
    "!!!!!!!!!!!!!!!!!!\n",
    "'''\n",
    "# little_epsilon: tolerable in-sample error\n",
    "# (not currently being used, the error never seems to reach low enough)\n",
    "little_epsilon = 0.1\n",
    "# eta: learning rate\n",
    "eta = 0.001\n",
    "# learning iterations\n",
    "iterations = 200\n",
    "\n",
    "# target data column: 1\n",
    "# input data column: 3\n",
    "#learn: P(Default|Balance)\n",
    "learn_class(\"Credit_Data.csv\", 3, 'Balance', 1, little_epsilon, eta, iterations)\n",
    "\n",
    "# target data column: 1\n",
    "# input data column: 4\n",
    "#learn: P(Default|Income)\n",
    "learn_class(\"Credit_Data.csv\", 4, 'Income (* 1000)', 1, little_epsilon, eta, iterations)\n",
    "\n",
    "#demonstrate correct classification: P(D|B)\n",
    "#P(Default|Balance)\n",
    "#P(Default|Income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
